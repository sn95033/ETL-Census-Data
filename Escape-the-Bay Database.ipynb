{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL of Census Data\n",
    "\n",
    "The \"Escape the Bay\" project was a success,  but generated 11 separate CSV Files!\n",
    "\n",
    "Since each analysis was performed separately,  it was difficult to be able to draw correlations between the datasets.\n",
    "\n",
    "The purpose of this ETL Homework is create a database were all the data can be stored, and queries can be written from dataset to dataset.\n",
    "\n",
    "Thus the Tasks will be:\n",
    "\n",
    "### 1) Extract data from 6 CSVs (there is some duplication of information) and import into Pandas\n",
    "\n",
    "### 2) Transform\n",
    "  #### A. Eliminate un-needed data and missing data\n",
    "  #### B. Harmonize the naming of the key cities in the analysis so the tables can be joined more easily\n",
    "  #### C. Based on the dataset size, determine whether to join the data in Pandas,  or in SQL\n",
    "  #### D. Output csv files into the SQL_data folder\n",
    "  \n",
    "### 3) Load\n",
    "   \n",
    " #### A. Create the Schema for the Escape-The-Bay Database using quickDBD\n",
    "\n",
    " #### B.  Create the tables in SQL\n",
    "\n",
    " #### C. Upload transformed data csv (from the SQL_Data folder) into a POSTGRESQL database\n",
    "\n",
    " #### D. Check the database and write a few sample queries using SQLALchemy;  \n",
    "\n",
    "### Document!\n",
    "   \n",
    "References:  The original data sources comes from Vanessa Oakes, Emily Todd, Stefan Zobrist and Rebecca Mih\n",
    "The API call comes from Emily Todd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census Flows Mapper outputs\n",
    "csv_path = \"./Resources/SF_All_OUT.csv\"\n",
    "sf_out_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Census Quick Facts CSV\n",
    "# The \"counties\" CSV contain information regarding:\n",
    "# Population demographics, Number of owner occupied housing, Median Value of owner occupied housing, Median Gross rent\n",
    "# Median Income, Total # of Employer Establishments, Total annual payroll, FIPs code\n",
    "\n",
    "CA_counties = pd.read_csv(\"./Resources/CA_counties.csv\")\n",
    "non_CA_counties = pd.read_csv(\"./Resources/non_CA_counties.csv\")\n",
    "marital_ca_df = pd.read_csv('./Resources/California - Marital 3.csv')\n",
    "marital_out_df = pd.read_csv('./Resources/Out of State - Marital.csv')\n",
    "\n",
    "# Census Advanced Fact Finder CSV from the American Community Survey (ACS)  \n",
    "#The \"income and mortgage\" CSV is a unique dataset that contains the distributions (bins) of the Total Household Income\n",
    "# the mortgage values, and the debt to income rati0,  by county\n",
    "ACS_data = pd.read_csv(\"./Resources/2017_income_mortgage.csv\")\n",
    "\n",
    "# Census API call and CSV creation\n",
    "# This API call creates the dataset for Median home values, Median Rental costs\n",
    "base_url = \"https://api.census.gov/data/2017/acs/acs1/profile\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform \n",
    "\n",
    "## Census Flows Mapper Data\n",
    "\n",
    "The U.S. Census has a very handy tool called Census Flows Mapper which automatically determines the outbound and inbound migrants from any given county\n",
    "\n",
    "https://flowsmapper.geo.census.gov/\n",
    "\n",
    "The data in the CSVs came from the output of that website\n",
    "\n",
    "Activities\n",
    "- Keep in mind that this csv will have the primary key through the county_name for the Sequel Database, which means the county_name observations all need to be spelled the same for each table to be uploaded.  In the future use the FIPS codes instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  First determine where people migrate to,  from San Francisco.  Look at those moving within CA\n",
    "\n",
    "#Top 5 in-CA counties\n",
    "ranked_sf_out_df = sf_out_df.sort_values(by='Total', ascending=False)\n",
    "sf_to_ca = ranked_sf_out_df.loc[ranked_sf_out_df[\"State Name\"] == \"California\",:]\n",
    "\n",
    "\n",
    "#Transorm the data -- Only take the top 5 destinations\n",
    "\n",
    "sftoca = sf_to_ca.iloc[:5,]\n",
    "\n",
    "sftoca = sftoca.rename(columns={\"Total\": \"# Migrated from SF County (2017)\"})\n",
    "\n",
    "sftoca['County Name'] = sftoca['County Name'].replace(\n",
    "    {'Alameda County': 'Alameda, CA', 'San Mateo County': 'San Mateo, CA', \"Contra Costa County\":'Contra Costa, CA',\n",
    "     \"Los Angeles County\":'Los Angeles, CA', 'Santa Clara County':'Santa Clara, CA'})\n",
    "\n",
    "\n",
    "#sftoca = sftoca.set_index([\"County Name\"])\n",
    "\n",
    "sftoca.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Top destinations moving outside of CA\n",
    "\n",
    "#Top 5 non-CA counties\n",
    "sf_not_ca = ranked_sf_out_df.loc[ranked_sf_out_df[\"State Name\"] != \"California\",:]\n",
    "sf_not_ca.head(5)\n",
    "\n",
    "#Transorm the data -- only take top 5 destinations\n",
    "sfnotca = sf_not_ca.iloc[:5,]\n",
    "sfnotca = sfnotca.rename(columns={\"Total\": \"# Migrated from SF County (2017)\"})\n",
    "\n",
    "# No filtering needed, keep the FIPs code for the SQL database sfnotca_summary = sf_not_ca.iloc[:5,3:7]\n",
    "sfnotca['County Name'] = sfnotca['County Name'].replace(\n",
    "    {'New York County': 'NY (Manhattan), NY', 'King County': 'King, WA', \"Multnomah County\":'Multnomah, OR',\n",
    "     \"Kings County\":'Kings (Brooklyn), NY', 'Cook County':'Cook, IL'})\n",
    "\n",
    "#sfnotca = sfnotca.set_index([\"County Name\"])\n",
    "\n",
    "sfnotca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two tables together for a single destinations file\n",
    "\n",
    "Destinations = pd.merge(sftoca,sfnotca,how='outer')\n",
    "\n",
    "\n",
    "#Destinations = Destinations.set_index([\"State/County FIPs\"])\n",
    "\n",
    "# Keep the FIPs ids as strings, so that they don't lose the 0 at the beginning\n",
    "\n",
    "# Add in San Francisco County to the table ??\n",
    "\n",
    "Destinations = Destinations.rename(columns = { 'State/County FIPS':'state_county_fips',\n",
    "                'State FIPS': 'state_fips', 'County FIPS':'county_fips',\n",
    "                'County Name':'county_name', 'State Name':'state_name',\n",
    "                '# Migrated from SF County (2017)':'number_migrated_2017',\n",
    "                'Margin of Error (+/-)': 'margin_of_error',\n",
    "                })\n",
    "\n",
    "\n",
    "Destinations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add San Francisco into the data to harmonize with other tables\n",
    "\n",
    "sfdata = pd.Series({'state_county_fips': '06075',\n",
    "                       'state_fips' : '06',\n",
    "                       'county_fips' : '075',\n",
    "                       'county_name' : 'San Francisco CA',\n",
    "                       'state_name' : 'California',\n",
    "                       'number_migrated_2017' : 0, \n",
    "                       'margin_of_error' : 0,\n",
    "                      })\n",
    "\n",
    "sfdata\n",
    "\n",
    "#test_dest = Destinations.append(sfdata, ignore_index = True)\n",
    "#test_dest\n",
    "Destinations = Destinations.append(sfdata, ignore_index = True)\n",
    "Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonize the Data types with other csvs\n",
    "cols = Destinations.columns\n",
    "cols\n",
    "\n",
    "Destinations[cols] = Destinations[cols].replace({'\\$': '', ',': '', '\\%':'', '\\\"': '', \"\\'\":''}, regex=True)\n",
    "Destinations[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Destinations = Destinations.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "#Destinations.index.name ='index'\n",
    "\n",
    "Destinations = Destinations.set_index('county_name')\n",
    "\n",
    "Destinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Destinations.to_csv('./SQL_data/destinations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dest_cols = list(Destinations.columns.values)\n",
    "Dest_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Destinations.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform From US Census \"QuickFacts Utility\"\n",
    "\n",
    "## Demographics, Age, Income, Median Housing, Median Rents, Commute, Employers\n",
    "\n",
    "\n",
    "Documentation of the Journey\n",
    "\n",
    "1. Use the graphical interface to input up to 6 locations (by city, county, state, etc) \n",
    "Reference:  https://www.census.gov/quickfacts/fact/table/US/PST045218\n",
    "\n",
    "** The only data cleaning done in Excel was to add (for out-of-state),  Travis County TX, (Austin is located there),  which was added manually to the \"non_CA_counties.csv\" files\n",
    "\n",
    "### Transform Data CleanUp Steps\n",
    "\n",
    "* Reduce the data size and clean up the naming (for easier reference later on)\n",
    "\n",
    "### Key commands used throughout, in order to clean\n",
    "\n",
    "* Dropping columns\n",
    "        df.drop(columns = ['column name'], inplace = True)-    Drop columns which have no important data\n",
    "* Dropping dta with Nans\n",
    "        df.dropna() - Drop rows with NaNs\n",
    "\n",
    "* Dropping rows\n",
    "        df = df[:x] - Drop rows, only keep x rows\n",
    "* Dropping rows, using the index position (to avoid a lot of typing of column names as found in Census data).\n",
    "* Afterwards reset the index if you are using it\n",
    "\n",
    "        Demo_summary = Demo_data.drop(Demo_data.index[[1,2,7,8,9,10,11,12,13,14,15,16,17,18,19,24,25,26,27,28,\n",
    "                                               29,30,31,32,33,34,37,38,39,40,41,42,45,46,53,54,55,56,57,58,59,60]])\n",
    "\n",
    "        Demo_summary = Demo_summary.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "* Adding in an index name          \n",
    "    Demographics.index.name ='county_name'\n",
    "    \n",
    "* Setting a new index based on a column, in this case the column name is county_name as the index in the Demograhics_df\n",
    " (was the best way to save into CSV for uploading to PostGres\n",
    "        Demographics_df = Demographics_df.set_index('county_name')\n",
    "\n",
    "* Renaming of columns - Rename the colums with shorter names so the plots look ok\n",
    "        df.rename(columns = { 'X': 'new x', 'Y': 'something else'})  \n",
    "        \n",
    "* Merge two dataframes -  merge the destinations both in CA and out of CA\n",
    "        Destinations = pd.merge(in_ca_df, out_ca_df, how = 'outer')\n",
    "        \n",
    "* List the columns, for purposes of cutting and pasting into other code\n",
    "    Dest_cols = list(Destinations.columns.values)  \n",
    "    \n",
    "* Replacing data in rows using the replace method\n",
    "  In this case, cleaning of characters ($,%, commas, quotes) from the data which caused the data type to be object rather than a numeric\n",
    "\n",
    "        cols = demo_df.columns\n",
    "        Demographics[cols] = demo_df[cols].replace({'\\$': '', '\\,': '', '\\%':'', '\\\"': '', \"\\'\": \"\"}, regex=True)\n",
    "    \n",
    "* Converting to numeric after cleaning\n",
    "    Demographics_df = demoT_summary_df.apply(pd.to_numeric, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CAcols = list(CA_counties.columns.values)\n",
    "CAcols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the raw data \n",
    "# Select the columns wanted\n",
    "\n",
    "#CA_df = CA_counties[['Fact',  'San Francisco County, California','Alameda County, California',\n",
    "#                    'San Mateo County, California', 'Contra Costa County, California',\n",
    "#                    'Los Angeles County, California','Santa Clara County, California',]]\n",
    "\n",
    "CA_counties.drop(columns = ['Fact Note'], inplace=True)\n",
    "CA_counties.drop(columns = ['Value Note for San Francisco County, California'], inplace=True)\n",
    "CA_counties.drop(columns = ['Value Note for Alameda County, California'], inplace=True)\n",
    "CA_counties.drop(columns = ['Value Note for San Mateo County, California'], inplace=True)\n",
    "CA_counties.drop(columns = ['Value Note for Contra Costa County, California'], inplace=True)\n",
    "CA_counties.drop(columns = ['Value Note for Los Angeles County, California'], inplace=True)\n",
    "CA_counties.drop(columns = ['Value Note for Santa Clara County, California'], inplace=True)\n",
    "\n",
    "non_CA_counties.drop(columns = ['Fact Note'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['San Francisco County, California'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Value Note for San Francisco County, California'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Value Note for King County, Washington'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Value Note for New York County (Manhattan Borough), New York'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Value Note for Multnomah County, Oregon'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Value Note for Kings County (Brooklyn Borough), New York'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Value Note for Cook County, Illinois'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Value Note for Travis County, Texas'], inplace=True)\n",
    "non_CA_counties.drop(columns = ['Travis County, Texas'], inplace=True)\n",
    "\n",
    "\n",
    "# Remove the rows which have NaNs,  doing inplace needed\n",
    "CA_counties.dropna(inplace=True)\n",
    "non_CA_counties.dropna(inplace=True)\n",
    "\n",
    "# Reset the index to keep everything in order, drop = True means that the original index will be discarded\n",
    "# Do this because we need to have one DF that shows the row number as a reference (later code)\n",
    "# Reference:  https://stackoverflow.com/questions/33165734/update-index-after-sorting-data-frame\n",
    "\n",
    "CA_counties.reset_index(drop=True, inplace=True)\n",
    "non_CA_counties.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Only keep the top 62 rows of data\n",
    "\n",
    "CA_counties = CA_counties[:62]\n",
    "non_CA_counties = non_CA_counties[:62]\n",
    "\n",
    "non_CA_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(non_CA_counties.columns.values)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_df = CA_counties.rename(columns = { 'San Francisco County, California': 'San Francisco CA',\n",
    "                                      'Alameda County, California': 'Alameda CA',\n",
    "                                      'San Mateo County, California': 'San Mateo CA', \n",
    "                                      'Contra Costa County, California': 'Contra Costa CA',\n",
    "                                      'Los Angeles County, California':'Los Angeles CA',\n",
    "                                      'Santa Clara County, California':'Santa Clara CA',})\n",
    "\n",
    "\n",
    "non_CA_df = non_CA_counties.rename(columns = {'New York County (Manhattan Borough), New York': 'NY (Manhattan) NY',\n",
    "                                             'King County, Washington': 'King WA', 'Multnomah County, Oregon': 'Multnomah OR',\n",
    "                                             'Kings County (Brooklyn Borough), New York': 'Kings (Brooklyn) NY',\n",
    "                                             'Cook County, Illinois': 'Cook IL', })\n",
    "\n",
    "\n",
    "CA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_CA_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the DataFrame to see what data to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demo_data = pd.merge(CA_df,non_CA_df, how = \"outer\" )\n",
    "#Demo_data2 = Demo_data.set_index(['Fact'])\n",
    "Demo_data.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Rows\n",
    "# Reference https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\n",
    "# proper usage of reset_index https://stackoverflow.com/questions/40755680/how-to-reset-index-pandas-dataframe-after-dropna-pandas-dataframe\n",
    "# drop = True re-assigns the same dataframe the values, with a new index\n",
    "\n",
    "Demo_summary = Demo_data.drop(Demo_data.index[[1,2,7,8,9,10,11,12,13,14,15,16,17,18,19,24,25,26,27,28,\n",
    "                                               29,30,31,32,33,34,37,38,39,40,41,42,45,46,53,54,55,56,57,58,59,60]])\n",
    "\n",
    "Demo_summary = Demo_summary.reset_index(drop=True)\n",
    "\n",
    "Demo_summary.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "Demographics = Demo_summary.set_index('Fact')\n",
    "\n",
    "Demographics.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoT = Demographics.T\n",
    "demoT\n",
    "demoT_cols=list(demoT.columns.values)\n",
    "demoT_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update the column titles to be easier to \n",
    "\n",
    "demoT_summary = demoT.rename(columns={'Population estimates, July 1, 2017,  (V2017)': 'population_estimate_2017', \n",
    "                                        'Population, Census, April 1, 2010': 'population_census_2010',\n",
    "                                        'Persons under 5 years, percent': 'age_under_5yrs_pct',\n",
    "                                        'Persons under 18 years, percent': 'age_under_18yrs_pct',\n",
    "                                        'Persons 65 years and over, percent': 'age_above_65yrs_pct',\n",
    "                                        'Median value of owner-occupied housing units, 2013-2017': 'median_home',\n",
    "                                        'Median selected monthly owner costs -with a mortgage, 2013-2017': 'med_monthly_cost_with_mortgage',\n",
    "                                        'Median selected monthly owner costs -without a mortgage, 2013-2017': 'med_monthly_cost_no_mortgage',\n",
    "                                        'Median gross rent, 2013-2017': 'median_monthly_rent',\n",
    "                                        'In civilian labor force, total, percent of population age 16 years+, 2013-2017': 'employment_pct',\n",
    "                                        'In civilian labor force, female, percent of population age 16 years+, 2013-2017': 'employment_females_pct',\n",
    "                                        'Mean travel time to work (minutes), workers age 16 years+, 2013-2017': \"mean_travel_time_to_work\",\n",
    "                                        'Median household income (in 2017 dollars), 2013-2017': 'median_household_income',\n",
    "                                        'Total employer establishments, 2016': 'number_of_employers_2016',\n",
    "                                        'Total employment, 2016':'total_employed_2016',\n",
    "                                        'Total annual payroll, 2016 ($1,000)':'annual_payroll_2016_1Kdollars',\n",
    "                                        'Total nonemployer establishments, 2016': 'total_nonemployers',\n",
    "                                        'All firms, 2012':'number_of_employers_2012',\n",
    "                                        'FIPS Code': 'state_county_fips',\n",
    "                                        'Total employment, percent change, 2015-2016': 'total_employment_pct_change_2015_2016',\n",
    "                          \n",
    "                                            })\n",
    "demoT_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to numerical values\n",
    "Because the raw data in the csv is formatted with $, %, or ',' Pandas will read all data as objects into the DataFrame\n",
    "\n",
    "Clean the entire table, the user can specify the specific data fact (row) they wish to use\n",
    "df.replace - Replace the %, $, , in the data to blank\n",
    "df.apply(pd.to_numeric()) -- now change the objects in each column into numerics, \"apply\" will apply to all cols\n",
    "Use errors = 'coerce' to force to an number. If there are alphanumerics, they will become 'NaN's and you will lose the text. In that case use errors = 'ignore'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numerics in the dataframe\n",
    "cols = demoT_summary.columns\n",
    "cols\n",
    "\n",
    "demoT_summary[cols] = demoT_summary[cols].replace({'\\$': '', '\\,': '', '\\%':'', '\\\"': '', \"\\'\": \"\"}, regex=True)\n",
    "demoT_summary[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demographics = demoT_summary.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "Demographics.index.name ='county_name'\n",
    "\n",
    "Demographics\n",
    "\n",
    "# Reference for how to set the index name https://stackoverflow.com/questions/18022845/pandas-index-column-title-or-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demographics.to_csv('./SQL_data/demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demo_cols=list(Demographics.columns.values)\n",
    "Demo_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demographics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income and Mortgage data from census FactFinder Advanced Search utility\n",
    "Reference:\n",
    "Use the US Census FactFinder - Advanced Search functionality to get detailed in the area of Employment (including income), Housing (including Mortgage information), and Population demographics.\n",
    "\n",
    "The utility is fairly easy to use -- but Warning - there is a LOT of data, and often times the data is repeated\n",
    "\n",
    "Recommendations: Use the filtering and editing functions on the Advanced Search, BEFORE creating your CSV file.\n",
    "\n",
    "Select all counties of interest first as a filter.\n",
    "Use the graphical interface to input as many locations as you want (by city, county, state, etc)\n",
    "Once you have selected all the key locations, you can save the query, which saves time if you are going to do other analyses later on\n",
    "\n",
    "Edit out to the minimal data you need.\n",
    "\n",
    "The census provides the data with calculated error, or percent error. Those can be filtered\n",
    "Remove any columns of data you don't need. It's difficult to change column names with very large datasets, so better to minimize the number of columns if possible\n",
    "\n",
    "The Income and Mortgage CSV in this file was filtered and edited on the Census website, with some additional description cleaning in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to load\n",
    "ACS_data = pd.read_csv(\"./Resources/2017_income_mortgage.csv\")\n",
    "\n",
    "ACS_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Columns to be more understandable\n",
    "ACS_data.columns = ['ID', \n",
    "                    'County',\n",
    "                    '% of Mortgages Valued at <$50K',\n",
    "                         '% of Mortgages Valued at $50-$99K',\n",
    "                         '% of Mortgages Valued at $100K-$299K',\n",
    "                         '% of Mortgages Valued at $300K-$499K',\n",
    "                         '% of Mortgages Valued at $500K-$749K',\n",
    "                         '% of Mortgages Valued at $750K-$999K',\n",
    "                         '% of Mortgages Valued at >$1M',\n",
    "                        'Median Value of Mortgages ($)',\n",
    "                        '% Household income <$10K',\n",
    "                        '% Household income $10K-$24K',\n",
    "                        '% Household income $25K-34K',\n",
    "                        '% Household income $35K-$49K',\n",
    "                        '% Household income $50K-$74K',\n",
    "                        '% Household income $75K-$99K',\n",
    "                        '% Household income $100K-$150K',\n",
    "                       '% Household income >$150K',\n",
    "                       '2017 Household Median Income ($)',\n",
    "                       'Ratio of Mortgage Value to Income, % <2',\n",
    "                       'Ratio of Mortgage Value to Income, % 2-2.9',\n",
    "                       'Ratio of Mortgage Value to Income %, 3-3.9',\n",
    "                        'Ratio of Mortgage Value to Income, % > 4.0',\n",
    "                        'ID2']\n",
    "ACS_data.drop(columns = ['ID'], inplace=True)  \n",
    "ACS_data.set_index('County', inplace=True)                 \n",
    "\n",
    "ACS_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACS_dataT = ACS_data.T\n",
    "\n",
    "# Reset the index to keep everything in order, drop = True means that the original index will be discarded\n",
    "# Do this because we need to have one DF that shows the row number as a reference (later code)\n",
    "# Reference:  https://stackoverflow.com/questions/33165734/update-index-after-sorting-data-frame\n",
    "\n",
    "#ACS_dataT.reset_index(inplace=True)\n",
    "\n",
    "# Remove the rows which have NaNs,  doing inplace needed\n",
    "ACS_dataT.dropna(inplace=True)\n",
    "\n",
    "# Only keep the top 25 rows of data\n",
    "ACS_dataT = ACS_dataT[:25]\n",
    "\n",
    "ACS_dataT.drop(columns = ['Geography'], inplace=True)  \n",
    "\n",
    "ACS_dataT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns, look at the DataFrame\n",
    "\n",
    "ACS_dataT = ACS_dataT.drop(columns = ['Travis County, Texas'])\n",
    "ACS_dataT = ACS_dataT.rename(columns={\"Geography\":\"Description\",\n",
    "                                      \"San Francisco County, California\": \"San Francisco CA\",\n",
    "                                 \"Alameda County, California\":\"Alameda CA\",\n",
    "                                 \"San Mateo County, California\":\"San Mateo CA\",\n",
    "                                 \"Contra Costa County, California\":\"Contra Costa CA\",\n",
    "                                \"Los Angeles County, California\":\"Los Angeles CA\",\n",
    "                                \"Santa Clara County, California\":\"Santa Clara CA\",\n",
    "                                      \"New York County, New York\":\"NY (Manhattan) NY\",\n",
    "                                 \"King County, Washington\":\"King WA\",\n",
    "                                \"Multnomah County, Oregon\":\"Multnomah OR\",\n",
    "                                \"Kings County, New York\":\"Kings (Brooklyn) NY\",\n",
    "                                \"Cook County, Illinois\":\"Cook IL\",\n",
    "                                            })\n",
    "\n",
    "\n",
    "ACS_dataT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns, look at the DataFrame\n",
    "\n",
    "\n",
    "\n",
    "#ACS_cleanup = ACS_dataT.rename(columns={\n",
    "#                                      \"San Francisco County, California\": \"San Francisco\",\n",
    "##                                 \"Alameda County, California\":\"Alameda\",\n",
    "#                               \"San Mateo County, California\":\"San Mateo\",\n",
    "#                                 \"Contra Costa County, California\":\"Contra Costa\",\n",
    "#                                \"Los Angeles County, California\":\"Los Angeles\",\n",
    "#                                \"Santa Clara County, California\":\"Santa Clara\",\n",
    "#                                      \"New York County, New York\": \"NY (Manhattan)\",\n",
    "#                                 \"King County, Washington\":\"King\",\n",
    "#                                \"Multnomah County, Oregon\":\"Multnomah\",\n",
    "#                                \"Kings County, New York\":\"Kings (Brooklyn)\",\n",
    "#                                \"Cook County, Illinois\":\"Cook\",\n",
    "#                                            })\n",
    "\n",
    "\n",
    "#ACS_cleanup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Financial = ACS_dataT.T\n",
    "Financial.index.name = 'county_name'\n",
    "Financial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the Facts into the index to get it out of the way since we don't need to clean the numbers in that column\n",
    "# Making a new DF ca_data,  so  you can always refer to ca_df to see the line number of the row\n",
    "#ca_data = CA_counties.set_index(\"Fact\")\n",
    "\n",
    "# Clean the $ and % and \" signs from multiple columns, first put the columns put them in a list\n",
    "# Reference:  https://stackoverflow.com/questions/38516481/trying-to-remove-commas-and-dollars-signs-with-pandas-in-python\n",
    "\n",
    "cols = Financial.columns\n",
    "\n",
    "# pass cols to df.replace(), specifying $,%\" and , to be replaced by blanks\n",
    "\n",
    "Financial[cols] = Financial[cols].replace({'\\$': '', ',': '', '\\%':'', '\\\"': ''}, regex=True)\n",
    "\n",
    "\n",
    "# convert all objects to numerics\n",
    "# reference:  https://stackoverflow.com/questions/36814100/pandas-to-numeric-for-multiple-columns\n",
    "# https://pandas.pydata.org/pandas-docslist(/stable/reference/api/pandas.to_numeric.html\n",
    "#cols = ACS_dataT.columns[ACS_dataT.dtypes.eq('object')]\n",
    "Financial = Financial[cols].apply(pd.to_numeric, errors='ignore')\n",
    "Financial.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Financial = Financial.rename(columns ={'ID2': 'state_county_fips'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup the column names\n",
    "\n",
    "Financial_cols = list(Financial.columns.values)\n",
    "Financial_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Financial = Financial.rename(columns = {\n",
    "    '% of Mortgages Valued at <$50K': 'mortgages_valued_lt_50K_pct',\n",
    " '% of Mortgages Valued at $50-$99K': 'mortgages_valued_50-99K_pct',\n",
    " '% of Mortgages Valued at $100K-$299K': 'mortgages_valued_100-299K_pct',\n",
    " '% of Mortgages Valued at $300K-$499K': 'mortgages_valued_300-499K_pct',\n",
    " '% of Mortgages Valued at $500K-$749K': 'mortgages_valued_500-749K_pct',\n",
    " '% of Mortgages Valued at $750K-$999K': 'mortgages_valued_750-999K_pct',\n",
    " '% of Mortgages Valued at >$1M': 'mortgages_valued_gt_1M_pct',\n",
    "  'Median Value of Mortgages ($)': 'median_mortgage_value',\n",
    " '% Household income <$10K': \"household_income_lt_10K\",\n",
    " '% Household income $10K-$24K': 'household_income_10-24K',\n",
    " '% Household income $25K-34K': 'household_income_25-34K',\n",
    " '% Household income $35K-$49K': 'household_income_35-49K_pct',\n",
    " '% Household income $50K-$74K': 'household_income_50-74K_pct',\n",
    " '% Household income $75K-$99K': 'household_income_75-99K_pct',\n",
    " '% Household income $100K-$150K': 'household_income_100-150K_pct',\n",
    " '% Household income >$150K': 'household_income_gt_150K_pct',\n",
    " '2017 Household Median Income ($)': 'median_household_income_2017',\n",
    " 'Ratio of Mortgage Value to Income, % <2': 'mortgage_to_income_Ratio_lt_2',\n",
    " 'Ratio of Mortgage Value to Income, % 2-2.9': 'mortgage_to_income_Ratio_2-3',\n",
    " 'Ratio of Mortgage Value to Income %, 3-3.9': 'mortgage_to_income_Ratio_3-4',\n",
    " 'Ratio of Mortgage Value to Income, % > 4.0': 'mortgage_to_income_Ratio_gt_4.0',\n",
    " 'State/County_FIPS': 'state_county_fips'\n",
    "\n",
    "})\n",
    "\n",
    "Financial.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Financial_cols = list(Financial.columns.values)\n",
    "Financial_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Financial.to_csv('./SQL_data/financial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Financial.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using US Census API to extract Home Ownership and Home Rental data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary for in-CA counties\n",
    "\n",
    "base_url = \"https://api.census.gov/data/2017/acs/acs1/profile\"\n",
    "\n",
    "ca_cty_name = [\"San Francisco\",\"Alameda\",\"San Mateo\",\"Contra Costa\",\"Los Angeles\",\"Santa Clara\"]\n",
    "ca_st_fips = [\"06\",\"06\",\"06\",\"06\",\"06\",\"06\"]\n",
    "ca_cty_fips = [\"075\",\"001\",\"081\",\"013\",\"037\",\"085\"]\n",
    "\n",
    "\n",
    "in_ca_dict = {\n",
    "    \"County Name\": ca_cty_name,\n",
    "    \"State_FIPS\": ca_st_fips,\n",
    "    \"County_FIPS\": ca_cty_fips\n",
    "}\n",
    "\n",
    "in_ca_df = pd.DataFrame(in_ca_dict)\n",
    "\n",
    "#dictionary for non-CA counties\n",
    "\n",
    "nonca_cty_name = ['NY (Manhattan)',\"King\",\"Multnomah\",\"Kings (Brooklyn)\",\"Cook\"]\n",
    "nonca_st_fips = [\"36\",\"53\",\"41\",\"36\",\"17\"]\n",
    "nonca_cty_fips = [\"061\",\"033\",\"051\",\"047\",\"031\"]\n",
    "\n",
    "non_ca_dict = {\n",
    "    \"County Name\": nonca_cty_name,\n",
    "    \"State_FIPS\": nonca_st_fips,\n",
    "    \"County_FIPS\": nonca_cty_fips\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "non_ca_df = pd.DataFrame(non_ca_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect median home values by county and add to data frames\n",
    "\n",
    "ca_med_home_val = []\n",
    "med_home_var = \"DP04_0089E\"\n",
    "    \n",
    "for county_id, state_id in zip(ca_cty_fips, ca_st_fips):\n",
    "    med_home_val = requests.get(f\"{base_url}?get={med_home_var}&for=county:{county_id}&in=state:{state_id}\").json()\n",
    "    ca_med_home_val.append(int(med_home_val[1][0]))\n",
    "    \n",
    "in_ca_df[\"Med_Home_Value\"] = ca_med_home_val\n",
    "in_ca_df.to_csv('./Extra/ca_home_value.csv')\n",
    "print(in_ca_df)\n",
    "\n",
    "non_ca_med_home_val = []\n",
    "med_home_var = \"DP04_0089E\"\n",
    "\n",
    "for county_id, state_id in zip(nonca_cty_fips, nonca_st_fips):\n",
    "    med_home_val = requests.get(f\"{base_url}?get={med_home_var}&for=county:{county_id}&in=state:{state_id}\").json()\n",
    "    non_ca_med_home_val.append(int(med_home_val[1][0]))\n",
    "    \n",
    "non_ca_df[\"Med_Home_Value\"] = non_ca_med_home_val\n",
    "non_ca_df.to_csv('./Extra/nonca_home_value.csv')\n",
    "print(non_ca_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#follow the same process for median gross rents\n",
    "\n",
    "ca_med_rent = []\n",
    "med_rent_var = \"DP04_0134E\"\n",
    "    \n",
    "for county_id, state_id in zip(ca_cty_fips, ca_st_fips):\n",
    "    med_rent = requests.get(f\"{base_url}?get={med_rent_var}&for=county:{county_id}&in=state:{state_id}\").json()\n",
    "    ca_med_rent.append(int(med_rent[1][0]))\n",
    "    \n",
    "in_ca_df[\"Med_Rent\"] = ca_med_rent\n",
    "\n",
    "in_ca_df.to_csv('./Extra/ca_rents.csv')\n",
    "print(in_ca_df)\n",
    "\n",
    "non_ca_med_rent = []\n",
    "med_rent_var = \"DP04_0134E\"\n",
    "\n",
    "for county_id, state_id in zip(nonca_cty_fips, nonca_st_fips):\n",
    "    med_rent= requests.get(f\"{base_url}?get={med_rent_var}&for=county:{county_id}&in=state:{state_id}\").json()\n",
    "    non_ca_med_rent.append(int(med_rent[1][0]))\n",
    "    \n",
    "non_ca_df[\"Med_Rent\"] = non_ca_med_rent\n",
    "non_ca_df.to_csv('./Extra/nonca_rents.csv')\n",
    "print(non_ca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#follow the same process for home owner rate\n",
    "\n",
    "ca_own_rate = []\n",
    "home_own_var = \"DP04_0046PE\"\n",
    "    \n",
    "for county_id, state_id in zip(ca_cty_fips, ca_st_fips):\n",
    "    own_rate = requests.get(f\"{base_url}?get={home_own_var}&for=county:{county_id}&in=state:{state_id}\").json()\n",
    "    ca_own_rate.append(float(own_rate[1][0]))\n",
    "    \n",
    "in_ca_df[\"Home Own Rate\"] = ca_own_rate\n",
    "in_ca_df.to_csv('./Extra/ca_homeowner_rates.csv')\n",
    "print(in_ca_df)\n",
    "\n",
    "nonca_own_rate = []\n",
    "home_own_var = \"DP04_0046PE\"\n",
    "    \n",
    "for county_id, state_id in zip(nonca_cty_fips, nonca_st_fips):\n",
    "    own_rate = requests.get(f\"{base_url}?get={home_own_var}&for=county:{county_id}&in=state:{state_id}\").json()\n",
    "    nonca_own_rate.append(float(own_rate[1][0]))\n",
    "    \n",
    "non_ca_df[\"Home Own Rate\"] = nonca_own_rate\n",
    "non_ca_df.to_csv('./Extra/nonca_homeowner_rates.csv')\n",
    "print(non_ca_df)\n",
    "non_ca_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Home = pd.merge(in_ca_df, non_ca_df, how = 'outer')\n",
    "\n",
    "# Rename the columns to harmonize to other tables\n",
    "Home = Home.rename(columns = {\n",
    "        'County Name':'county_name', \"Home Own Rate\":\"home_ownership_rate_pct\",\n",
    "        'State_FIPS': 'state_fips', 'County_FIPS': 'county_fips', 'Med_Home_Value': 'med_home_value',\n",
    "        'Med_Rent': 'med_rent',\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "Home['county_name'] = Home['county_name'].replace(\n",
    "                      {\"San Francisco\": \"San Francisco CA\",\n",
    "                                 \"Alameda\":\"Alameda CA\",\n",
    "                                 \"San Mateo\":\"San Mateo CA\",\n",
    "                                 \"Contra Costa\":\"Contra Costa CA\",\n",
    "                                \"Los Angeles\":\"Los Angeles CA\",\n",
    "                                \"Santa Clara\":\"Santa Clara CA\",\n",
    "                                      \"NY (Manhattan)\":\"NY (Manhattan) NY\",\n",
    "                                 \"King\":\"King WA\",\n",
    "                                \"Multnomah\":\"Multnomah OR\",\n",
    "                                \"Kings (Brooklyn)\":\"Kings (Brooklyn) NY\",\n",
    "                                \"Cook\":\"Cook IL\",   \n",
    "                      })\n",
    "\n",
    "Home = Home.set_index('county_name')\n",
    "            \n",
    "Home\n",
    "\n",
    "# Rename the columns to harmonize with other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Home.to_csv('./SQL_data/home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_cols = list(Home.columns.values)\n",
    "home_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Home.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Loading into SQL using POSTGRES\n",
    "\n",
    "#### 1. Create the ERD and Scheme for SQL\n",
    "A. Use the https://app.quickdatabasediagrams.com tool to created the ERD. \n",
    "B. Type in all the column names exactly as shown in the CSVs, along with their datatypes (VARCHAR(255), INT, FLOAT)\n",
    "C. Make sure that the datatypes match to the actual datatypes in the CSV\n",
    "D. Assign the PK (primary key)  for each table\n",
    "E. Assign the FK (foreign keys) for each table\n",
    "\n",
    "#### 2. Export from quickdatabasediagrams.com to PostGres from the Export dropdown menu to create a schema file.\n",
    "A. Take a picture of the ERD and save it in your directory for this project\n",
    "B. Go to your downloads, and open the latest generated POSTGRES.sql file for your scheme.\n",
    "C. Save the file into your directory, as both as .sql and a .txt file\n",
    "\n",
    "\n",
    "#### 3. Load into POSTGRES\n",
    "\n",
    "A. Open PGAdmin\n",
    "B. If you haven't do so already, create a new database, by right-clicking on the Databases in the left windowpane\n",
    "C. Select \"Create Database\", and fill in the new database name in the pop-up window\n",
    "D. When the new Database is created, you should see the new database name within your list of databases in your left window pane, click on it to \"activate it.\n",
    "\n",
    "E. Rightclick on the new database, and select \"Query Tool\",  and it should open a Query pame\n",
    "\n",
    "F. Now load in the datatables from your schema.  You copy and paste from the quickdatdiagrams .sql file that you generated in Step 2C above.  \n",
    "\n",
    "G. You may need to debug errors in file loading,  so you probably want to a \"Drop Table\" statement at the befinning of the query.  The syntax is:  DROP TABLE IF EXISTS tablename;\n",
    "\n",
    "You will need to order the dropping of tables, by doing the dependent ones first,  and the independent table as the last in the sequence, or you will get an error.\n",
    "\n",
    "H. Fix any errors in creating tables.  After the tables are created,  you should see them in your database, in the left pane.\n",
    "\n",
    "I. Load in the CSVs:  Go to each table.  It is best to load the Table which is independent, so that the primary key can be loaded.  Right click on table name.  \n",
    "\n",
    "J. A new box will open,  Click on the \"Export\" that is highlighted, to toggle to \"Import\"\n",
    "\n",
    "K. Select the proper csv filename, by clicking on the ... to the right and navigate to your directory and click on the csv you want\n",
    "\n",
    "L. Click on the Header no to toggle to Yes  (You want Header Yes to be highlighted)\n",
    "\n",
    "J Click on the delimiter and choose the comma option  (,)\n",
    "\n",
    "K. Import the CSV.  Fix errors in your data  (non-matching names, etc).  Rinse and repeat for all the tables.\n",
    "\n",
    "Now you are ready to run some queries and then move onto SQLalchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned Thus Far\n",
    "\n",
    "1. Putting data into databases takes a lot of thinking before doing\n",
    "\n",
    "2.  There are a lot of data names manipulations, that you need to do, in order to harmonize the tables to each other.   The key thing is that the variable (column) which is planned to be the primary key, ought to contain a list of all the observations, in the parent table  (the table which does not have the foreign key dependency).\n",
    "\n",
    "3. There will be a lot of row name replacements (using the replace function) to harmonize the keys on all the tables,  but particularly the primary key and matching foreigh keys connect to the primary key.\n",
    "\n",
    "4. I've undertaken to avoid spaces in columns names.  Not sure if that's a good thing, but I didn't want to find out later.  Thus using underscore _ for all column names using the rename column function.  \n",
    "\n",
    "5. Naming columns from Census data can be tedious, because the census sometimes has really long column names.  However, it's probably worth it\n",
    "\n",
    "6. With Census data in CSV form, especially, you do have to check the datatypes especially using the built-in Census queries, like Advanced Fact Finder and QuickFinder. Often the data will have characters like $, %,  which will then force the data to be read in as an object.  That means one has to clean these characters and then re-process the data as numeric (pd_numeric). Other tools such as API calls and Census Flows Mapper did not have this problem, so you have to pay attention to how the data was read in\n",
    "\n",
    "7.  The Census FIPS code could be a primary key -- but the harmonization of tables can be difficult if  the data is read in by different methods (see 6. above). The state and county FIPS code is a concatenation of the individual codes -- including the leading zero, especially needed on the county FIPS.  In this interaction, I will have to go back and re-work this section, as well as the naming of the variables (I have not harmonice the data types for the FIPS codes across tables, which is a problem for the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install psycopg2 sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SqlAlchemy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postgres://username:password@localhost:port/database_name\n",
    "\n",
    "engine = create_engine('postgres://postgres:12345678@localhost:65183/ETL-Escape-the_Bay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = engine.execute(\"SELECT * FROM destinations\")\n",
    "for record in data:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = engine.execute(\"SELECT * FROM home\")\n",
    "for record in data:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = engine.execute(\"SELECT * FROM demographics\")\n",
    "for record in data:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = engine.execute(\"SELECT * FROM financial\")\n",
    "for record in data:\n",
    "    print(record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
